<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--script async src="https://www.googletagmanager.com/gtag/js?id=UA-41105501-1"></script-->
  <script type="text/javascript" src="/theme/js/refs_v1.js"> </script> 
  <script type="text/javascript" src="/theme/js/bibtexParse.js"> </script> 
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-41105501-1');
  </script>

  <meta charset="utf-8" />
  <title>Numerical Computing Notes (on-going post)</title>
  <meta name="author" content="Swair" />
  <link rel="stylesheet" type="text/css" href="/theme/tufte-css/tufte.css" />
  <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />
  <link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png" />
  <link rel="shortcut icon" href="/static/favicon.ico" />
  <link rel="icon" type="image/png" href="/static/favicon.png" />

  <!-- OpenGraph Info -->


  <style type="text/css">
    .blue {
      color: blue;
    }
  </style>


</head>

<body>
<header>
  <h1 class="websitetitle"><a href="">Working Notes</a></h1>
  <!-- <nav class="navmenu" id="navmenu">
    <li><a class="navitemlink" href="/archives.html">Archive</a></li>
   </nav> -->
   <nav class="navmenu" id="navmenu">
    <li><a class="navitemlink" href="/pages/about.html">About</a></li>
    <li><a class="navitemlink" href="/">Blog</a></li>
    <li><a class="navitemlink" href="/archives.html">Archive</a></li>
   </nav>
 </header>
<article>

<header class="post-header">
<h1> <a rel="bookmark"
   title="Numerical Computing Notes (on-going post) «Working Notes»"
   href="/numerical-computing-notes-on-going-post.html">
   Numerical Computing Notes (on-going post)
</a>
</h1>
<p class="subtitle"></p>
<p class="articlemeta"><label for="numerical-computing-notes-on-going-post" class="margin-toggle"> ⊕</label><input type="checkbox" id="numerical-computing-notes-on-going-post" class="margin-toggle" />
<span class="marginnote"><br/>
By Swair.
<br/><br />
Category: <a href="/category/numerical-linear-algebra-ml-julia-python.html">Numerical-Linear-Algebra, ML, Julia, Python</a>
<br />
<br /><br/>
<time datetime="2020-04-10T00:00:00-07:00">Fri 10 April 2020</time>

</span>
</p>



</header><div id="TOC"> </div>
<p>I did a fair bit of numerical linear algebra in my gradschool. Most of the time
I used libraries written by my adviser Haim Schweitzer over the years. Some
of the algorithms I came across were wonderful, e.g. Lanczos Iterations,
Power Method. As I never had a proper course in Numerical Analysis
I feel the need to understand some things which I missed. I will keep addind the
notes and references here. I will also include some Julia or Python code.</p>
<p>[-] sample1 [-].
[@@sample1]
[@sample1]</p>
<h2>Errors in numerical computations and condition number</h2>
<p>There are various ways erros can creep into numerical computations,
e.g. discretization, modelling, approximation etc.
In many numerical computations we are trying to approximate a solution.
If we don't know the exact solution then how can we estimate the errors?
For example say, we want to find a root $x_<em>$ of $f(x) = 0$,
and our approxiate solution is $x_{est}$, how do we measure error
$|x_</em> - x_{est}|$ without knowing  $x_*$?</p>
<p><strong>Forward Error</strong>: Use a proxy for the actual error as an estimate for the real
error. E.g. use $|f(x_<em>) - f(x_{est})|$ as a proxy for $|x_</em> - x_{est}|$.</p>
<p><strong>Backward Error</strong>: Estimate how much change in the problem is necessary
so that the approximate solution becomes exact? To understand this consider
solving the system of linear equations $Ax = b$. Assume that we have an
approximate algorithm which gives us $x_{est}$. We can compute $Ax_{est} = b_{est}$,
the change $b - b_{est}$ is a change in $b$ (which is part of the problem
<em>formulation</em> ) can make the approximate solution exact. This is called the
backward error. In this example computing the forward error would be to compute,
$|A^{-1} b - x_{est}|$ which requires a matrix inversion
(and that computation may itself have errors of its own). Many times its
easier to compute the backward errors than the forward errors.</p>
<h3>Condition Number</h3>
<p>A problem is well conditioned if low for backward errors imply low forward errors.
So a small change in the problem (e.g. values of matrix $A$ in $Ax = b$) will
have small change the solution.</p>
<p>In numerical linear algebra we compute the condition number of a matrix to judge
its <em>well-behavedness</em>. The condition number of a matrix A with respect to
a given norm $| \cdot |$ is,</p>
<p class="equation">
\begin{equation}
    \text{cond}(A) = \frac{\| A \|}{ \| A^{-1} \|}
\end{equation}
</p>

<h2>Gradient Descent</h2>
<div class="highlight"><pre><span></span><code><span class="k">Error</span><span class="o">:</span><span class="w"> </span><span class="n">UndefVarError</span><span class="o">:</span><span class="w"> </span><span class="n n-Quoted">`julia`</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">defined</span>
</code></pre></div>

<p>Lets compare the solution of <code>A\b</code> to the one achieved by
<code>GradientDescent</code>.</p>
<div class="highlight"><pre><span></span><code><span class="n">M</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">);</span>

<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">M</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">M</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="n">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">b</span><span class="o">&#39;</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span>
<span class="n">∇f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="n">x̂</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">minimize</span><span class="p">(</span><span class="n">GradientDescent</span><span class="p">(</span><span class="mf">0.05</span><span class="p">),</span><span class="w"> </span><span class="n">f</span><span class="p">,</span><span class="w"> </span><span class="n">∇f</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="mi">5000</span><span class="p">)</span>

<span class="n">print</span><span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">x̂</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">A</span><span class="o">\</span><span class="n">b</span><span class="p">))</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="k">Error</span><span class="o">:</span><span class="w"> </span><span class="n">UndefVarError</span><span class="o">:</span><span class="w"> </span><span class="n n-Quoted">`GradientDescent`</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">defined</span>
</code></pre></div>

<h2>Conjugate Gradients</h2>
<div class="highlight"><pre><span></span><code><span class="k">using</span><span class="w"> </span><span class="n">LinearAlgebra</span><span class="p">,</span><span class="w"> </span><span class="n">IterativeSolvers</span>

<span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">diagm</span><span class="p">(</span><span class="mi">100</span><span class="o">:</span><span class="mi">199</span><span class="p">)</span>
<span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ones</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">history</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cg</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="n">log</span><span class="o">=</span><span class="nb">true</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">[</span><span class="ss">:resnorm</span><span class="p">],</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nb">false</span><span class="p">)</span>
</code></pre></div>

<p><img alt="" src="figures/NumericalAnalysis_4_1.png"></p>
<hr>
<p>References:</p>
<p>[1] Trefethen, Lloyd N., and David Bau III.
    Numerical linear algebra. Vol. 50. Siam, 1997.</p>
<p>[2] Kochenderfer, Mykel J., and Tim A. Wheeler.
    Algorithms for optimization. Mit Press, 2019.</p>
<p>[3]  Solomon, Justin. Numerical algorithms: methods for computer vision,
    machine learning, and graphics. CRC press, 2015.</p>
</article>
<!--div id="disqus_thread"></div-->
<script>
doTOC();
doNotes();
</script>


<!-- <footer>
  <p>Powered by <a href="http://pelican.readthedocs.org">Pelican</a></p>
</footer> -->
<!--script id="dsq-count-scr" src="//isaythings.disqus.com/count.js" async></script-->
</body>

</html>